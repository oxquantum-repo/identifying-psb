{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import os\n",
    "import imgaug.augmenters as iaa\n",
    "from tqdm.notebook import tqdm\n",
    "from training_utils import (\n",
    "    Triangles,\n",
    "    build_batches,\n",
    "    get_net_optimiser_scheduler_criterion,\n",
    ")\n",
    "import training_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from importlib import reload\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Augmentation/')\n",
    "sys.path.append('../Simulator/')\n",
    "import simulation\n",
    "import augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define name of runs and create folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_type = \"finfets\"\n",
    "\n",
    "NAME_OF_RUN = \"2021XXXX_only_simulated_data_\" + device_type + \"/\"\n",
    "\n",
    "path = \"data/\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "path = path + NAME_OF_RUN\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "path_networks = \"data/saved_networks/\" + NAME_OF_RUN\n",
    "if not os.path.exists(path_networks):\n",
    "    os.makedirs(path_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on available ressources we can use either a GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load real data and reshape to have uniform pixel dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../Data/processed_data/\" + device_type + \"_imgs.npy\", allow_pickle=True)\n",
    "y = np.load(\"../Data/processed_data/\" + device_type + \"_labels.npy\")\n",
    "names = np.load(\"../Data/processed_data/\" + device_type + \"_names.npy\")\n",
    "device_names = np.load(\"../Data/processed_data/\" + device_type + \"_device_names.npy\")\n",
    "\n",
    "resizer = iaa.Resize([100, 100])\n",
    "\n",
    "new_X = []\n",
    "for element in X:\n",
    "    el0 = resizer.augment_image(image=element[0])\n",
    "    el1 = resizer.augment_image(image=element[1])\n",
    "    im = augmentations.normalise([el0, el1])\n",
    "    new_X.append(im)\n",
    "X = np.array(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training only with simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "n_episodes = 128  # None #only make a single batch\n",
    "n_repetitions = 10\n",
    "\n",
    "n_imgs_per_batch = 25000\n",
    "n_augmentations = 2  # 10\n",
    "\n",
    "print_every_n_epoch = 33\n",
    "\n",
    "# test_devices=np.unique(device_names)\n",
    "test_devices = [\n",
    "    \"Tuor6A_chiplet_5_device_C\",\n",
    "    \"Tuor2E_chiplet_10_device_J\",\n",
    "    \"Tuor6A_chiplet_6_device_E\",\n",
    "    \"Tuor6A_chiplet_7_device_A\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_only_sim = training_utils.get_results_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating a new batch with 25000 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdcbce6e62a4997a801aaf6dd322a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschuff/Documents/Projects/PSB_paper/Code_for_PSB_publication/Training/../Simulator/simulation.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp((x - mu) * beta))\n",
      "/Users/jonasschuff/Documents/Projects/PSB_paper/Code_for_PSB_publication/Training/../Simulator/simulation.py:48: RuntimeWarning: overflow encountered in divide\n",
      "  denominator = tc**2 * (2 + gamma_d / gamma_s) + 0.25 * gamma_d**2 + epsilon**2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1357baf8c6b741d0a7cd71f1489a7232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschuff/opt/anaconda3/envs/test2/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/jonasschuff/opt/anaconda3/envs/test2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695f5db509474e1480e1fe8afc56f4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m net(X_train_minibatch)\n\u001b[1;32m     47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train_minibatch)\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Record the loss and learning rate at each epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test2/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test2/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This loop simulates, augments and trains the neural network for a number of repetitions\n",
    "for rep in range(n_repetitions):\n",
    " \n",
    "    # Simulate a new batch of images and corresponding labels\n",
    "    print(\"simulating a new batch with\", n_imgs_per_batch, \"samples\")\n",
    "    X_train_sim, y_train_sim = simulation.simulate(n_imgs_per_batch)\n",
    "    X_train_sim = np.array(X_train_sim)\n",
    "    y_train_sim = np.array(y_train_sim, dtype=int)\n",
    "\n",
    "    # Augment the simulated batch for a number of times and store the augmented images and labels\n",
    "    X_train_new = []\n",
    "    y_train_new = []\n",
    "    for n_aug in tqdm(range(n_augmentations)):\n",
    "        _X_train = augmentations.augment_batch_mp(\n",
    "            X_train_sim, shear_and_stretch=False, n_workers=20\n",
    "        )\n",
    "        X_train_new.append(_X_train)\n",
    "        y_train_new.append(y_train_sim)\n",
    "\n",
    "    # Reshape the augmented images and labels to match the required input shape for the model\n",
    "    X_train_sim = np.array(X_train_new)\n",
    "    y_train_sim = np.array(y_train_new)\n",
    "    X_train_sim = X_train_sim.reshape(\n",
    "        (-1, X_train_sim.shape[-3], X_train_sim.shape[-2], X_train_sim.shape[-1])\n",
    "    )\n",
    "    y_train_sim = y_train_sim.reshape(-1)\n",
    "\n",
    "    # Prepare the training set for the model\n",
    "    X_train = X_train_sim\n",
    "    y_train = y_train_sim\n",
    "    dataset = Triangles(imgs=X_train, labels=y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=n_episodes, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Initialize the model, optimizer, learning rate scheduler and loss function\n",
    "    net, optimizer, scheduler, criterion = get_net_optimiser_scheduler_criterion(device)\n",
    "\n",
    "    # Train the model for a number of epochs\n",
    "    loss_history = []\n",
    "    lr_history = []\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_train_minibatch = sample_batched[\"image\"].to(device).float()\n",
    "            y_train_minibatch = sample_batched[\"label\"].to(device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X_train_minibatch)\n",
    "            loss = criterion(outputs, y_train_minibatch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Record the loss and learning rate at each epoch\n",
    "        loss_history.append(loss.item())\n",
    "        lr_history.append(optimizer.param_groups[0][\"lr\"])\n",
    "        scheduler.step(loss.item())\n",
    "\n",
    "        # Print the loss and learning rate every few epochs\n",
    "        if epoch % print_every_n_epoch == print_every_n_epoch - 1:\n",
    "            print(\"[%d] loss: %.7f\" % (epoch + 1, loss.item()))\n",
    "            print(\"learning rate now:\", optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(\n",
    "        net.state_dict(), path_networks + \"/only_simulator_rep_\" + str(rep) + \".pth\"\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the full dataset\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_full = net(torch.FloatTensor(X).to(device))\n",
    "        predicted_full = torch.max(outputs_full.data, 1).indices.detach().cpu()\n",
    "        m = nn.Softmax()\n",
    "        scores_full = m(outputs_full).detach().cpu().numpy()\n",
    "\n",
    "    # Compute the scores for each class in the dataset\n",
    "    scores_full = scores_full[:, 1]\n",
    "\n",
    "    # Record the results of the model's predictions and report them\n",
    "    results_only_sim = training_utils.record_results(\n",
    "        results_only_sim,\n",
    "        predicted_full,\n",
    "        scores_full,\n",
    "        y,\n",
    "        device_names=device_names,\n",
    "        triangle_names=names,\n",
    "    )\n",
    "    training_utils.report_results(results_only_sim)\n",
    "\n",
    "    # Save the results\n",
    "    pickle.dump(results_only_sim, open(path + \"/results_only_sim.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with only experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_OF_RUN = \"2021XXXX_crossdevice_only_real_data_\" + device_type\n",
    "\n",
    "path = \"data/\"\n",
    "\n",
    "path = path + NAME_OF_RUN\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "path_networks = \"data/saved_networks/\" + NAME_OF_RUN\n",
    "if not os.path.exists(path_networks):\n",
    "    os.mkdir(path_networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_samples = 50000\n",
    "\n",
    "chunksize = 10\n",
    "\n",
    "print_every_n_epoch = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the results\n",
    "results_only_real_data = training_utils.get_results_dict()\n",
    "\n",
    "# The main loop for running the experiment multiple times\n",
    "for rep in range(n_repetitions):\n",
    "    print(\"This is rep\", rep)\n",
    "\n",
    "    # Initialize lists to store predictions, scores, labels, names and device names for each repetition\n",
    "    predicted = []\n",
    "    scores = []\n",
    "    y_test_this_rep = []\n",
    "    _names = []\n",
    "    _device_names = []\n",
    "\n",
    "    fold = 0\n",
    "    # For each device in the test set, create a fold\n",
    "    for test_device_name in test_devices:\n",
    "        print(\"testing\", test_device_name)\n",
    "        \n",
    "        # Create an index for training and testing based on the device name\n",
    "        if test_device_name == \"Tuor6A_chiplet_5_device_C\":\n",
    "            test_index = np.logical_or(\n",
    "                device_names == \"Tuor6A_chiplet_5_device_C_cooldown_1\",\n",
    "                device_names == \"Tuor6A_chiplet_5_device_C_cooldown_2\",\n",
    "            )\n",
    "        else:\n",
    "            test_index = device_names == test_device_name\n",
    "        train_index = np.logical_not(test_index)\n",
    "        \n",
    "        # Split the data into training and testing sets based on the index\n",
    "        _names.append(names[test_index])\n",
    "        _device_names.append(device_names[test_index])\n",
    "        X_train_real, X_test_real = X[train_index], X[test_index]\n",
    "        y_train_real, y_test_real = y[train_index], y[test_index]\n",
    "        names_train, names_test = names[train_index], names[test_index]\n",
    "        y_train_real = np.array(y_train_real, dtype=int)\n",
    "\n",
    "        # Determine the number of augmentations required to reach the desired total number of samples\n",
    "        n_augmentations_real = n_total_samples // (len(X_train_real) * chunksize)\n",
    "\n",
    "        # Repeat the training data to match the number of augmentations\n",
    "        X_train_real = np.repeat(X_train_real, n_augmentations_real, axis=0)\n",
    "        y_train_real = np.repeat(y_train_real, n_augmentations_real, axis=0)\n",
    "\n",
    "        # Augment the real data\n",
    "        # print(\"augmenting the real data this many times: \", n_augmentations_real)\n",
    "        # print(\"# data in real training data\", len(X_train_real))\n",
    "        X_train_real_new = []\n",
    "        y_train_real_new = []\n",
    "        for n_aug in tqdm(range(chunksize + 1)):\n",
    "            _X_train_real = augmentations.augment_batch_mp(X_train_real, n_workers=20)\n",
    "            X_train_real_new.append(_X_train_real)\n",
    "            y_train_real_new.append(y_train_real)\n",
    "        X_train_real = np.array(X_train_real_new)\n",
    "        y_train_real = np.array(y_train_real_new)\n",
    "\n",
    "        # Reshape the augmented data to match the required input shape for the model\n",
    "        X_train_real = X_train_real.reshape(\n",
    "            (-1, X_train_real.shape[-3], X_train_real.shape[-2], X_train_real.shape[-1])\n",
    "        )\n",
    "        y_train_real = y_train_real.reshape(-1)\n",
    "\n",
    "        # Randomly shuffle the training data and take the first n_total_samples entries      \n",
    "        idx = np.random.permutation(len(X_train_real))\n",
    "        X_train = X_train_real[idx]\n",
    "        y_train = y_train_real[idx]\n",
    "        X_train = X_train[:n_total_samples]\n",
    "        y_train = y_train[:n_total_samples]\n",
    "\n",
    "        # print(\"len total data\", len(X_train))\n",
    "\n",
    "        # Create a dataset and dataloader for the training data\n",
    "        dataset = Triangles(imgs=X_train, labels=y_train)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, batch_size=n_episodes, shuffle=True, num_workers=0\n",
    "        )\n",
    "\n",
    "        # Calculate class weights to handle class imbalance\n",
    "        class_weights = sklearn.utils.class_weight.compute_class_weight(\n",
    "            \"balanced\", [0, 1], y_train\n",
    "        )\n",
    "        class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "        \n",
    "        # Create a new model, optimizer, scheduler, and loss function for this repetition\n",
    "        net, optimizer, scheduler, criterion = get_net_optimiser_scheduler_criterion(\n",
    "            device, class_weights=class_weights\n",
    "        )\n",
    "\n",
    "        # Initialize lists to store loss and learning rate history for this repetition\n",
    "        loss_history = []\n",
    "        lr_history = []\n",
    "        \n",
    "        # Train the model for n_epochs        \n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            for i_batch, sample_batched in enumerate(dataloader):\n",
    "                X_train_minibatch = sample_batched[\"image\"].to(device).float()\n",
    "                y_train_minibatch = sample_batched[\"label\"].to(device).long()\n",
    "\n",
    "                # Forward pass, calculate loss, backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(X_train_minibatch)\n",
    "                loss = criterion(outputs, y_train_minibatch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Record the loss and learning rate for this epoch\n",
    "            loss_history.append(loss.item())\n",
    "            lr_history.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "            scheduler.step(loss.item())\n",
    "            \n",
    "            # Print loss every print_every_n_epoch epochs\n",
    "            if epoch % print_every_n_epoch == print_every_n_epoch - 1:\n",
    "                print(\"[%d] loss: %.7f\" % (epoch + 1, loss.item()))\n",
    "                print(\"learning rate now:\", optimizer.param_groups[0][\"lr\"])\n",
    "                \n",
    "        # Evaluate the model after training\n",
    "        net.eval()\n",
    "        outputs = net(torch.FloatTensor(X_test_real).to(device))\n",
    "        _predicted = torch.max(outputs.data, 1).indices.detach().cpu()\n",
    "        m = nn.Softmax()\n",
    "        _scores = m(outputs).detach().cpu().numpy()\n",
    "\n",
    "        # Store the predictions, scores, and true labels for this repetition\n",
    "        predicted.append(_predicted)\n",
    "        scores.append(_scores[:, 1])\n",
    "        y_test_this_rep.append(y_test_real)\n",
    "\n",
    "        # print(sklearn.metrics.confusion_matrix(y_test_real, _predicted, labels=[0, 1]))\n",
    "        net.train()\n",
    "\n",
    "    # Combine the predictions, scores, and labels from all folds\n",
    "    _names = np.hstack(_names)\n",
    "    _device_names = np.hstack(_device_names)\n",
    "    predicted = np.hstack(predicted)\n",
    "    scores = np.hstack(scores)\n",
    "    y_test_this_rep = np.hstack(y_test_this_rep)\n",
    "\n",
    "    \n",
    "    # Record the results of this repetition in the results dictionary\n",
    "    results_only_real_data = training_utils.record_results(\n",
    "        results_only_real_data,\n",
    "        predicted,\n",
    "        scores,\n",
    "        y_test_this_rep,\n",
    "        _device_names,\n",
    "        _names,\n",
    "    )\n",
    "\n",
    "    # Print the results for this repetition\n",
    "    training_utils.report_results(results_only_real_data)\n",
    "    \n",
    "    # Save the results to a file\n",
    "    pickle.dump(\n",
    "        results_only_real_data, open(path + \"/results_only_real_data.pkl\", \"wb\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with mixed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_OF_RUN = \"2021XXXX_mixed_data_\" + device_type\n",
    "\n",
    "path = \"data/\"\n",
    "\n",
    "path = path + NAME_OF_RUN\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "path_networks = \"data/saved_networks/\" + NAME_OF_RUN\n",
    "if not os.path.exists(path_networks):\n",
    "    os.mkdir(path_networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "n_episodes = 128  # None #only make a single batch\n",
    "n_repetitions = 10\n",
    "\n",
    "img_size = (100, 100)\n",
    "n_imgs_per_batch = 12500\n",
    "n_augmentations = 2  # 10\n",
    "chunksize = 10\n",
    "\n",
    "print_every_n_epoch = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the results\n",
    "results_mixed_data = training_utils.get_results_dict()\n",
    "\n",
    "# This loop is running the training process multiple times (n_repetitions)\n",
    "for rep in range(len(results_mixed_data[\"AUC\"]), n_repetitions):\n",
    "    print(\"this is rep\", rep)\n",
    "    \n",
    "    # Simulating a batch of images for training\n",
    "    print(\"simulating a new batch with\", n_imgs_per_batch, \"samples\")\n",
    "    X_train_sim, y_train_sim = simulation.simulate(n_imgs_per_batch)\n",
    "    X_train_sim = np.array(X_train_sim)\n",
    "    y_train_sim = np.array(y_train_sim, dtype=int)\n",
    "\n",
    "    # Augment the simulated batch of data\n",
    "    X_train_new = []\n",
    "    y_train_new = []\n",
    "    for n_aug in tqdm(range(n_augmentations)):\n",
    "        _X_train = augmentations.augment_batch_mp(\n",
    "            X_train_sim, shear_and_stretch=False, n_workers=20\n",
    "        )\n",
    "        X_train_new.append(_X_train)\n",
    "        y_train_new.append(y_train_sim)\n",
    "        \n",
    "    # Reshape the augmented data to match the required input shape for the model\n",
    "    X_train_sim = np.array(X_train_new)\n",
    "    y_train_sim = np.array(y_train_new)\n",
    "    X_train_sim = X_train_sim.reshape(\n",
    "        (-1, X_train_sim.shape[-3], X_train_sim.shape[-2], X_train_sim.shape[-1])\n",
    "    )\n",
    "    y_train_sim = y_train_sim.reshape(-1)\n",
    "\n",
    "    # print(\"augmented sim shape\", X_train_sim.shape)\n",
    "    \n",
    "    # Initialize lists to store predictions, scores, labels, names and device names for each repetition\n",
    "    predicted = []\n",
    "    scores = []\n",
    "    y_test_this_rep = []\n",
    "    _names = []\n",
    "    _device_names = []\n",
    "\n",
    "    fold = 0\n",
    "\n",
    "    for test_device_name in test_devices:\n",
    "        print(\"testing\", test_device_name)\n",
    "    \n",
    "        # There is one device that shows up under two names\n",
    "        if test_device_name == \"Tuor6A_chiplet_5_device_C\":\n",
    "            test_index = np.logical_or(\n",
    "                device_names == \"Tuor6A_chiplet_5_device_C_cooldown_1\",\n",
    "                device_names == \"Tuor6A_chiplet_5_device_C_cooldown_2\",\n",
    "            )\n",
    "        else:\n",
    "            test_index = device_names == test_device_name\n",
    "        train_index = np.logical_not(test_index)\n",
    "        \n",
    "        # Split the data into training and testing sets based on the index\n",
    "        _names.append(names[test_index])\n",
    "        _device_names.append(device_names[test_index])\n",
    "        X_train_real, X_test_real = X[train_index], X[test_index]\n",
    "        y_train_real, y_test_real = y[train_index], y[test_index]\n",
    "        names_train, names_test = names[train_index], names[test_index]\n",
    "        y_train_real = np.array(y_train_real, dtype=int)\n",
    "\n",
    "        # Determine the number of augmentations required to reach the desired total number of samples\n",
    "        n_augmentations_real = len(X_train_sim) // (len(X_train_real) * chunksize)\n",
    "\n",
    "        # Repeat the training data to match the number of augmentations\n",
    "        X_train_real = np.repeat(X_train_real, n_augmentations_real, axis=0)\n",
    "        y_train_real = np.repeat(y_train_real, n_augmentations_real, axis=0)\n",
    "\n",
    "        # print(\"augmenting the real data this many times: \", n_augmentations_real)\n",
    "        # print(\"# data in real training data\", len(X_train_real))\n",
    "        X_train_real_new = []\n",
    "        y_train_real_new = []\n",
    "        # Augment the real data\n",
    "        for n_aug in tqdm(range(chunksize + 1)):\n",
    "            _X_train_real = augmentations.augment_batch_mp(X_train_real, n_workers=20)\n",
    "            X_train_real_new.append(_X_train_real)\n",
    "            y_train_real_new.append(y_train_real)\n",
    "        \n",
    "        # Reshape the augmented data to match the required input shape for the model\n",
    "        X_train_real = np.array(X_train_real_new)\n",
    "        y_train_real = np.array(y_train_real_new)\n",
    "        X_train_real = X_train_real.reshape(\n",
    "            (-1, X_train_real.shape[-3], X_train_real.shape[-2], X_train_real.shape[-1])\n",
    "        )\n",
    "        y_train_real = y_train_real.reshape(-1)\n",
    "    \n",
    "        # Randomly shuffle the training data and take the first n_total_samples entries\n",
    "        idx = np.random.permutation(len(X_train_real))\n",
    "        X_train_real = X_train_real[idx]\n",
    "        y_train_real = y_train_real[idx]\n",
    "        X_train_real = X_train_real[: len(X_train_sim)]\n",
    "        y_train_real = y_train_real[: len(X_train_sim)]\n",
    "\n",
    "        print(\"len sim data\", len(X_train_sim), \", len real data\", len(X_train_real))\n",
    "        \n",
    "        # Combine the simulated and real training data\n",
    "        X_train = np.vstack([X_train_sim, X_train_real])\n",
    "        y_train = np.hstack([y_train_sim, y_train_real])\n",
    "\n",
    "        print(\"len total data\", len(X_train))\n",
    "        \n",
    "        # Create a PyTorch DataLoader for the combined training data\n",
    "        dataset = Triangles(imgs=X_train, labels=y_train)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, batch_size=n_episodes, shuffle=True, num_workers=0\n",
    "        )\n",
    "\n",
    "        # Compute class weights to handle class imbalance in the training data\n",
    "        class_weights = sklearn.utils.class_weight.compute_class_weight(\n",
    "            \"balanced\", [0, 1], y_train\n",
    "        )\n",
    "        class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "        \n",
    "        # Get the network, optimizer, learning rate scheduler, and loss function\n",
    "        net, optimizer, scheduler, criterion = get_net_optimiser_scheduler_criterion(\n",
    "            device, class_weights=class_weights\n",
    "        )\n",
    "\n",
    "        # Initialize lists to store loss and learning rate history for this repetition\n",
    "        loss_history = []\n",
    "        lr_history = []\n",
    "        \n",
    "        # Train the model for n_epochs\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            for i_batch, sample_batched in enumerate(dataloader):\n",
    "                X_train_minibatch = sample_batched[\"image\"].to(device).float()\n",
    "                y_train_minibatch = sample_batched[\"label\"].to(device).long()\n",
    "                \n",
    "                # Forward pass, calculate loss, backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(X_train_minibatch)\n",
    "                loss = criterion(outputs, y_train_minibatch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Record the loss and learning rate for this epoch\n",
    "            loss_history.append(loss.item())\n",
    "            lr_history.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "            scheduler.step(loss.item())\n",
    "            \n",
    "            # Print loss every print_every_n_epoch epochs\n",
    "            if epoch % print_every_n_epoch == print_every_n_epoch - 1:\n",
    "                print(\"[%d] loss: %.7f\" % (epoch + 1, loss.item()))\n",
    "                print(\"learning rate now:\", optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "        # Evaluate the model after training\n",
    "        net.eval()\n",
    "        outputs = net(torch.FloatTensor(X_test_real).to(device))\n",
    "        _predicted = torch.max(outputs.data, 1).indices.detach().cpu()\n",
    "        m = nn.Softmax()\n",
    "        _scores = m(outputs).detach().cpu().numpy()\n",
    "\n",
    "        # Store the predictions, scores, and true labels for this repetition\n",
    "        predicted.append(_predicted)\n",
    "        scores.append(_scores[:, 1])\n",
    "        y_test_this_rep.append(y_test_real)\n",
    "        \n",
    "        # Print the confusion matrix for this repetition\n",
    "        print(sklearn.metrics.confusion_matrix(y_test_real, _predicted, labels=[0, 1]))\n",
    "        \n",
    "        # Switch the model back to training mode for the next repetition\n",
    "        net.train()\n",
    "    \n",
    "    # Combine the predictions, scores, and labels from all folds\n",
    "    _names = np.hstack(_names)\n",
    "    _device_names = np.hstack(_device_names)\n",
    "    predicted = np.hstack(predicted)\n",
    "    scores = np.hstack(scores)\n",
    "    y_test_this_rep = np.hstack(y_test_this_rep)\n",
    "\n",
    "    # Record the results of this repetition of the training and testing process\n",
    "    results_mixed_data = training_utils.record_results(\n",
    "        results_mixed_data, predicted, scores, y_test_this_rep, _device_names, _names\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    training_utils.report_results(results_mixed_data)\n",
    "\n",
    "    # Save the results to a pickle file\n",
    "    pickle.dump(results_mixed_data, open(path + \"/results_mixed_data.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
